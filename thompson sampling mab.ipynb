{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T12:13:47.270055Z",
     "start_time": "2018-11-23T12:13:47.266030Z"
    }
   },
   "source": [
    "# a Bernoulli MAB \"solved\" with Thompson Sampling\n",
    "Thompson Sampling is a very interesting and _simple_ algorithm, for learning the best arm in a Multi-Arm-Bandit scenario. This notebook is intended as both a tutorial and a testbed for experimentation. The code simulating the bandit itself (the true world) is run from library python code, but all the rest is self-contained.\n",
    "\n",
    "The inspiration is credited to https://peterroelants.github.io/posts/multi-armed-bandit-implementation/, which you are encouraged to follow before proceeding here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Imports and Google Colaboratory setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T13:27:22.142277Z",
     "start_time": "2018-11-23T13:27:22.136243Z"
    }
   },
   "outputs": [],
   "source": [
    "github_repo_name = 'bandits'\n",
    "github_url = 'https://github.com/matanster/' + github_repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T13:27:23.089042Z",
     "start_time": "2018-11-23T13:27:22.144767Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import random\n",
    "\n",
    "if get_ipython().__class__.__module__ == \"google.colab._shell\":\n",
    "    print('running in google colab')\n",
    "    \n",
    "    !git clone $github_url # clone the repo so that the python files of the repo are available to the runtime\n",
    "    exec('import ' + github_repo_name + '.python_lib.bandit as bd')\n",
    "    exec('from ' + github_repo_name + '.python_lib.util import unique_argmax')\n",
    "    exec('from ' + github_repo_name + '.python_lib.plot import interval_binner, plot_series, plot_serie')\n",
    "\n",
    "\n",
    "else:\n",
    "    print('running locally')\n",
    "    \n",
    "    import python_lib.bandit as bd\n",
    "    from python_lib.util import unique_argmax\n",
    "    from python_lib.plot import interval_binner, plot_series, plot_serie\n",
    "    \n",
    "    #import ipycache # consult https://github.com/rossant/ipycache#usage for usage\n",
    "    #%load_ext ipycache \n",
    "    #cache_file = 'cache-file' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Algorithm and Our Simulation Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Thompson Sampling algorithm is very simple. It is essentially a Bayesian updating process with a small twist. We maintain and associate a beta distribution per arm of the Multi-Arm-Bandit and then iterate: \n",
    "\n",
    "At each round of interaction:\n",
    "\n",
    "+ we sample each of these distributions, obtaining a sample value per arm\n",
    "+ we then choose the arm that yielded the highest sample value \n",
    "+ we then choose that arm, and observe a reward given by it \n",
    "+ we update the distribution associated to that arm (review [here](https://www.youtube.com/watch?v=hKYvZF9wXkk) or elsewhere for the update rule we apply). \n",
    "\n",
    "You can see that this is a strictly a Bayesian process ― we continuously update our prior from the posterior, but only for one arm per round. At each round we choose the arm to play by the argmax of a random sample from each arm's prior distribution; we play that arm and update its associated distribution according to the reward we observed from it. This shall all be evident in the flow of the notebook that ensues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's our function for selecting the arm to choose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T13:27:23.094506Z",
     "start_time": "2018-11-23T13:27:23.090618Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def thompson_select(beta_priors, bandits):   \n",
    "    ''' samples from the prior distribution per bandit, and selects the argmax bandit '''\n",
    "\n",
    "    beta_dist_samples = list(map(lambda k: np.random.beta(*beta_priors[k], size=1)[0], range(bandits.num_of_bandits)))\n",
    "    selection = np.argmax(beta_dist_samples)\n",
    "    \n",
    "    if selection.size == 1:\n",
    "        return selection.flat[0]\n",
    "    else:\n",
    "        return random.choice(selection) # tie break the argmax by random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the complete algorithm running _i_ rounds of interaction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T13:27:23.253513Z",
     "start_time": "2018-11-23T13:27:23.097474Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def simulate(bandits, beta_priors, n):\n",
    "    ''' unleash online learning over n turns, keeping a record of every turn '''\n",
    "    \n",
    "    beta_priors = beta_priors.copy()\n",
    "    \n",
    "    round_records = []\n",
    "\n",
    "    for i in range(n):\n",
    "\n",
    "        round_record = dict(priors=list(beta_priors))\n",
    "\n",
    "        k = thompson_select(beta_priors, bandits)\n",
    "        outcome = bandits.pull(k)\n",
    "\n",
    "        # the posterior computation here is specific to our simple case. \n",
    "        if outcome == 1:\n",
    "            beta_priors[k] = (beta_priors[k][0]+1, beta_priors[k][1]) # increment the alpha param\n",
    "        elif outcome == 0:\n",
    "            beta_priors[k] = (beta_priors[k][0], beta_priors[k][1]+1) # increment the beta param\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        round_record.update(dict(k=k, outcome=outcome, posteriors=list(beta_priors)))\n",
    "\n",
    "        round_records.append(round_record)\n",
    "        \n",
    "    return pd.DataFrame.from_records(round_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate success, we should look at multiple runs of the simulation, because the algorithm is probabilistic. So we also have a function here, that will run the simulation _n_ times for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T13:27:23.322931Z",
     "start_time": "2018-11-23T13:27:23.256739Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def simulate_n_times(bandits, beta_priors, turns, simulations):\n",
    "    ''' run the same simulation m times, accumulating a record of every simulation's every turn '''\n",
    "    simulation_dfs = []\n",
    "    for i in range(simulations):\n",
    "        simulation_df = simulate(bandits, beta_priors, turns)\n",
    "        simulation_dfs.append(simulation_df)\n",
    "    return simulation_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions above will accumulate the data from all runs using pandas objects, for extra safe exploration of the results. In the same vein, the following functions will manipulate the accumulated experiment data, so that we can e.g. have the average of each turn _i_ across all of the _n_ simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T13:27:23.390298Z",
     "start_time": "2018-11-23T13:27:23.326609Z"
    },
    "hideOutput": true
   },
   "outputs": [],
   "source": [
    "def compute_cross_simulation_stats(simulation_dfs, columns, stat_specs):\n",
    "    ''' computes turn-wise stats across simulation dataframes.\n",
    "        this is like building a data cube and aggregating the desired statistics on its new axis. '''\n",
    "    \n",
    "    # an intermediary dataframe where the index is the rounds, and each cell is the array \n",
    "    # of values across all simulations, for each original column\n",
    "    rounds_df = pd.DataFrame(index=simulation_dfs[0].index, columns=columns)\n",
    "    for column in columns:          \n",
    "        for round in range(simulation_dfs[0].shape[0]):\n",
    "            round_values = [simulation_df.iloc[round][column] for simulation_df in simulation_dfs]\n",
    "            rounds_df[column].iloc[round] = round_values\n",
    "\n",
    "    # the result dataframe where the index is the rounds, there is a column for every column+stat combo,\n",
    "    # and the values are the implied statistics\n",
    "    rounds_stats_df = pd.DataFrame(index=rounds_df.index, columns=[column + '-' + stat_spec['name'] for column in columns for stat_spec in stat_specs])\n",
    "    for round in range(simulation_dfs[0].shape[0]):\n",
    "        for column in columns:          \n",
    "            for stat_spec in stat_specs:\n",
    "                rounds_stats_df.iloc[round][column + '-' + stat_spec['name']] = \\\n",
    "                    stat_spec['fn'](rounds_df.iloc[round][column])\n",
    "        \n",
    "    return rounds_stats_df\n",
    "\n",
    "\n",
    "def get_cross_simulation_stats(simulation_dfs):\n",
    "    ''' computes the average and std per turn, for items of interest '''\n",
    "    return compute_cross_simulation_stats(simulation_dfs, ['k', 'outcome'], [dict(name='avg', fn=np.mean), dict(name='std', fn=np.std)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T13:11:23.822699Z",
     "start_time": "2018-11-23T13:11:23.817525Z"
    }
   },
   "source": [
    "## Experiment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T13:00:52.148887Z",
     "start_time": "2018-11-23T13:00:52.142878Z"
    }
   },
   "source": [
    "### world setup\n",
    "Let's now setup the real world which our algorithm should simulate against; that is simply the parameters of the bandits. In our imaginary world, each bandit is a Bernoulli bandit, having a single parameter which is the probability of that bandit emitting a success signal when pulled. Our simulation functions reach out for an implementation of a Bernoulli bandit to simulate this world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T13:27:23.479281Z",
     "start_time": "2018-11-23T13:27:23.393731Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "bandits = bd.BanditsVec(list(map(bd.Bernoulli, [0.03, 0.3, 0.4, 0.8, 0.9, 0.95])), bd.Bernoulli.best_bandit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T13:07:34.718419Z",
     "start_time": "2018-11-23T13:07:34.712112Z"
    }
   },
   "source": [
    "\n",
    "### priors initialization\n",
    "We now initialize our beta priors to the uniform distribution. Lacking any outside information we have no reason to set them up any differently. Our notebook drives both the real world and the algorithm that runs against it. Obviously, it encodes the probabilities of the arms just one cell up, but we're after simulating how an agnostic algorithm would work..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T13:27:23.539460Z",
     "start_time": "2018-11-23T13:27:23.482989Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "beta_priors = list(map(lambda bandit: (1,1), range(bandits.num_of_bandits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T13:17:38.490261Z",
     "start_time": "2018-11-23T13:17:38.486328Z"
    }
   },
   "source": [
    "## Go algorithm go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T13:28:22.570466Z",
     "start_time": "2018-11-23T13:27:23.542907Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# %%cache $cache_file simulation_dfs, round_stats\n",
    "simulation_dfs = simulate_n_times(bandits, beta_priors, 200,1000)\n",
    "round_stats = get_cross_simulation_stats(simulation_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T13:20:07.043369Z",
     "start_time": "2018-11-23T13:20:07.039814Z"
    }
   },
   "source": [
    "### What did we get?\n",
    "As stated, our experiment runs _i_ rounds of interaction, _n_ times over from scratch, because each run will have different results. The expectation and variance for each round across the simulations, will give us the concise picture of how the experiment went."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T13:29:43.111995Z",
     "start_time": "2018-11-23T13:29:42.978016Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_series('reward per turn over {} simulations of {} rounds each'.format(len(simulation_dfs), simulation_dfs[0].shape[0]),\n",
    "            'turn', 'reward',            \n",
    "            round_stats,\n",
    "            [dict(column='outcome-avg', display_name='mean reward'), dict(column='outcome-std', display_name='std')], \n",
    "            lines=False,\n",
    "            marker_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T13:29:52.009921Z",
     "start_time": "2018-11-23T13:29:51.882782Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_series('arm selection per turn over {} simulations of {} rounds each'.format(len(simulation_dfs), simulation_dfs[0].shape[0]),\n",
    "            'turn', 'arm',            \n",
    "            round_stats,\n",
    "            [dict(column='k-avg', display_name='mean arm'), dict(column='k-std', display_name='std')], \n",
    "            lines=False,\n",
    "            marker_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T16:21:54.345106Z",
     "start_time": "2018-11-11T16:21:54.340439Z"
    }
   },
   "source": [
    "### What did our algorithm learn?\n",
    "We look at the distributions of the means of the posterior parameters of each arm as they stand at the end of the simulation runs. Explicitly put, we take the final mean of the alpha and beta per arm over all simulations, and plot those beta distributions. Following some aggregation code, the plot follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T13:28:27.487862Z",
     "start_time": "2018-11-23T13:28:22.883956Z"
    }
   },
   "outputs": [],
   "source": [
    "final_alpha_posteriors = pd.DataFrame(\n",
    "    index=range(len(simulation_dfs)),\n",
    "    columns = [arm for arm in range(bandits.num_of_bandits)])\n",
    "\n",
    "final_beta_posteriors = pd.DataFrame(\n",
    "    index=range(len(simulation_dfs)),\n",
    "    columns = [arm for arm in range(bandits.num_of_bandits)])\n",
    "    \n",
    "for i, simulation_df in enumerate(simulation_dfs):\n",
    "    for arm in range(bandits.num_of_bandits):\n",
    "        final_alpha_posteriors.iloc[i][arm] = simulation_df.tail(1).iloc[0]['posteriors'][arm][0]\n",
    "        final_beta_posteriors.iloc[i][arm]  = simulation_df.tail(1).iloc[0]['posteriors'][arm][1]\n",
    "        \n",
    "mean_alpha_posteriors = final_alpha_posteriors.mean()\n",
    "mean_beta_posteriors  = final_beta_posteriors.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T13:28:28.177132Z",
     "start_time": "2018-11-23T13:28:27.489497Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0,1,1000)\n",
    "arm_estimates_df = pd.DataFrame(index=x)\n",
    "for arm in range(bandits.num_of_bandits):   \n",
    "    beta_distribution = scipy.stats.beta(mean_alpha_posteriors[arm], mean_beta_posteriors[arm])\n",
    "    arm_estimates_df[arm] = [beta_distribution.pdf(i) for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T13:28:28.517035Z",
     "start_time": "2018-11-23T13:28:28.203600Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_series('final estimation of the probability density function per arm,<br>(averaged over all simulations)', '', '',\n",
    "            arm_estimates_df, list(range(bandits.num_of_bandits)), \n",
    "            lines=True, fill=True, marker_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T13:28:28.202301Z",
     "start_time": "2018-11-23T13:28:28.178595Z"
    }
   },
   "outputs": [],
   "source": [
    "params_df = pd.DataFrame(index = pd.Index(range(bandits.num_of_bandits), name='arm'))\n",
    "params_df['true param'] = pd.Series(map(lambda bandit: bandit.success_prob, bandits.bandits))\n",
    "params_df['mean(α)']    = pd.Series(mean_alpha_posteriors)\n",
    "params_df['mean(β)']    = pd.Series(mean_beta_posteriors)\n",
    "params_df['std(α)']     = pd.Series(final_alpha_posteriors.std())\n",
    "params_df['std(β)']     = pd.Series(final_beta_posteriors.std())\n",
    "display(params_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## TODO:\n",
    "\n",
    "+ optimize the simulation functions by using numpy directly or otherwise\n",
    "+ normalize the beta distributions into a probability distribution, at least for the plot (trivial)\n",
    "+ fancy: make a dash page/app that chooses simulation runs dynamically from the simulations cube"
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "66px",
    "width": "194px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
